{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcca7d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import torch\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision import utils, transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e4f354",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd3e8fa",
   "metadata": {},
   "source": [
    "### 1) Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa933ad9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adni_num = \"1\"\n",
    "test_data_suffix = 'multi-input'\n",
    "experiment_type = f\"{test_data_suffix}_XTEST\"\n",
    "num_folds = 5\n",
    "epochs = 2\n",
    "\n",
    "all_columns = ['AGE','PTGENDER','ADAS11', 'MMSE', 'FAQ', \\\n",
    "               'RAVLT_immediate', 'RAVLT_learning', 'RAVLT_forgetting', \\\n",
    "               'CDRSB', 'APOE4']\n",
    "\n",
    "required_columns = ['AGE','PTGENDER','APOE4']\n",
    "experiment_name = 'img_' + ('_'.join(required_columns)).lower()\n",
    "print(f\"{experiment_name}\")\n",
    "\n",
    "myseed = 1\n",
    "torch.manual_seed(myseed)\n",
    "np.random.seed(myseed)\n",
    "num_classes=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b66714f",
   "metadata": {},
   "source": [
    "#### Retrieve img filenames and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b054889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying the zip file name\n",
    "data_path = f\"a{adni_num}\"\n",
    "\n",
    "img_df_filename=f\"adni{adni_num}_paths.pkl\"\n",
    "filename=os.path.join(data_path, img_df_filename)\n",
    "img_df=pd.read_pickle(filename)  \n",
    "\n",
    "print(f\"Final data has {len(img_df)}\")\n",
    "img_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1ea0ea",
   "metadata": {},
   "source": [
    "#### Retrieve ADNI table, with normalized values (ADNI_ready.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb9b90b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = \"ADNI_csv/\"\n",
    "filename = \"ADNI_ready.csv\"\n",
    "\n",
    "adni_tabular = pd.read_csv(os.path.join(data_path, filename))\n",
    "adni_tabular.head()\n",
    "\n",
    "\n",
    "# Print \n",
    "print(f\"ALL adni has {len(adni_tabular)} entries\")\n",
    "print(f\"Class distribution is organized as follow:\")\n",
    "print(f\"\\n {adni_tabular['labels'].value_counts()}\")\n",
    "\n",
    "adni_tabular=adni_tabular[adni_tabular['SRC']==f\"ADNI{adni_num}\"]\n",
    "\n",
    "print(f\"Adni{adni_num} has {len(adni_tabular)} entries\")\n",
    "print(f\"Class distribution is organized as follow:\")\n",
    "print(f\"Final:\\n {adni_tabular['labels'].value_counts()}\")\n",
    "\n",
    "\n",
    "## Check for duplicated rows\n",
    "dup = adni_tabular[adni_tabular.duplicated()]\n",
    "if not dup.empty:\n",
    "    print(f\"WARNING: Dataframe contains duplicated rows!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b5e8af",
   "metadata": {},
   "source": [
    "#### Merge the images and tabular dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba00f6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only ADNI2 subjects\n",
    "adni = pd.merge( left=img_df, right=adni_tabular, how=\"inner\", on=\"PTID\", \n",
    "                      suffixes=(\"_x\", \"_y\"),copy=False, indicator=False, validate=\"one_to_one\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be0ce85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Class distribution is organized as follow:\")\n",
    "print(f\"Final:\\n {adni['labels'].value_counts()}\")\n",
    "adni.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d7f4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for subjects with missing images/tabular if and only IF there is a mismatch between classes counts among the 2 df\n",
    "\n",
    "images_pk = img_df['PTID'].tolist()\n",
    "adni_pk = adni_tabular['PTID'].tolist()\n",
    "\n",
    "intersection = list(set(images_pk) - set(adni_pk))\n",
    "#print(intersection)\n",
    "for ptid in intersection:\n",
    "    \n",
    "    if ptid in images_pk:\n",
    "        print(f\"{ptid} Missing tabular\")\n",
    "\n",
    "    elif ptid in adni_pk:\n",
    "        print(f\"{ptid} Missing Image\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c203d2ff",
   "metadata": {},
   "source": [
    "### 2) Dataset creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713ead29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgDataset(Dataset):\n",
    "    \"\"\"Tabular and Image dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, adni_df, required_columns=required_columns):\n",
    "        self.adni = adni_df\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.adni)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        line = self.adni.iloc[idx, 0:]\n",
    "        # Get Label\n",
    "        y = line['labels']\n",
    "\n",
    "        # Get tabular\n",
    "        tabular = line[required_columns]\n",
    "        tabular = torch.DoubleTensor(tabular)\n",
    "        \n",
    "        # Get image\n",
    "        image = nib.load(line['IMG_PATH'])\n",
    "        image = image.get_fdata() \n",
    "        #image = image[..., :3]\n",
    "        image = transforms.functional.to_tensor(image)\n",
    "        image = image.unsqueeze(dim=0)\n",
    "        \n",
    "        return image, tabular, y\n",
    "\n",
    "img_data = ImgDataset(adni_df=adni)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d309420d",
   "metadata": {},
   "source": [
    "### Detach test set and use remaining data for train-val k-fold split (further down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce539489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labels = adni['labels'].tolist()\n",
    "# Split data into train+val and test set indexes\n",
    "tv_idx, test_idx = train_test_split(np.arange(len(labels)), test_size=0.1,shuffle=True,stratify=labels)\n",
    "\n",
    "# Create train+val dataframe and show class balance\n",
    "adni_tv = adni.iloc[tv_idx]\n",
    "print(adni_tv.groupby([\"labels\"]).count())\n",
    "tv_data = ImgDataset(adni_df=adni_tv)\n",
    "\n",
    "# Create test dataframe and show class balance\n",
    "adni_test = adni.iloc[test_idx]\n",
    "print(adni_test.groupby([\"labels\"]).count())\n",
    "test_data = ImgDataset(adni_df=adni_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87feb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "\n",
    "if True:\n",
    "    torch.save(test_data, f'test_adni{adni_num}_{test_data_suffix}.pt')\n",
    "    saved_test = torch.load(f'test_adni{adni_num}_{test_data_suffix}.pt')\n",
    "   \n",
    "    print(f\"{test_data[i][0].size()}, label = {test_data[i][1]}\")\n",
    "    len(saved_test[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70de888",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{test_data[i][0].size()}, tabular = {test_data[i][1]}, label = {test_data[i][2]}\")\n",
    "\n",
    "len(test_data[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bacb7a",
   "metadata": {},
   "source": [
    "### 3. Model: img+tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aeae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inplanes():\n",
    "    return [64, 128, 256, 512]\n",
    "\n",
    "\n",
    "def conv3x3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv3d(in_planes,\n",
    "                     out_planes,\n",
    "                     kernel_size=3,\n",
    "                     stride=stride,\n",
    "                     padding=1,\n",
    "                     bias=False)\n",
    "\n",
    "\n",
    "def conv1x1x1(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv3d(in_planes,\n",
    "                     out_planes,\n",
    "                     kernel_size=1,\n",
    "                     stride=stride,\n",
    "                     bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = conv3x3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = conv1x1x1(in_planes, planes)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.conv2 = conv3x3x3(planes, planes, stride)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.conv3 = conv1x1x1(planes, planes * self.expansion)\n",
    "        self.bn3 = nn.BatchNorm3d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, block_inplanes, \\\n",
    "                 n_input_channels=3, conv1_t_size=7, \\\n",
    "                 conv1_t_stride=1, no_max_pool=False, \\\n",
    "                 shortcut_type='B', widen_factor=1.0, \\\n",
    "                 n_classes=400, img_contribution=10, \\\n",
    "                 tabular_val=10, tabular_contribution=10):\n",
    "        super().__init__()\n",
    "\n",
    "        block_inplanes = [int(x * widen_factor) for x in block_inplanes]\n",
    "\n",
    "        self.in_planes = block_inplanes[0]\n",
    "        self.no_max_pool = no_max_pool\n",
    "\n",
    "        self.conv1 = nn.Conv3d(n_input_channels,\n",
    "                               self.in_planes,\n",
    "                               kernel_size=(conv1_t_size, 7, 7),\n",
    "                               stride=(conv1_t_stride, 2, 2),\n",
    "                               padding=(conv1_t_size // 2, 3, 3),\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(self.in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, block_inplanes[0], layers[0],\n",
    "                                       shortcut_type)\n",
    "        self.layer2 = self._make_layer(block,\n",
    "                                       block_inplanes[1],\n",
    "                                       layers[1],\n",
    "                                       shortcut_type,\n",
    "                                       stride=2)\n",
    "        self.layer3 = self._make_layer(block,\n",
    "                                       block_inplanes[2],\n",
    "                                       layers[2],\n",
    "                                       shortcut_type,\n",
    "                                       stride=2)\n",
    "        self.layer4 = self._make_layer(block,\n",
    "                                       block_inplanes[3],\n",
    "                                       layers[3],\n",
    "                                       shortcut_type,\n",
    "                                       stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc = nn.Linear(block_inplanes[3] * block.expansion, img_contribution)\n",
    "        \n",
    "        #qui ho cambiato da num_classes a 10, per far cat con testo\n",
    "        #TESTO\n",
    "        self.relu = nn.ReLU()\n",
    "        self.ln1 = nn.Linear(tabular_val, 50) #23 sono le colonne in input\n",
    "        self.ln2 = nn.Linear(50, 50)\n",
    "        self.ln3 = nn.Linear(50, tabular_contribution)\n",
    "        self.ln4 = nn.Linear(tabular_contribution+img_contribution, n_classes) #20 perchÃ¨ 10 derivano da img e 10 da tab\n",
    "        \n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                nn.init.kaiming_normal_(m.weight,\n",
    "                                        mode='fan_out',\n",
    "                                        nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _downsample_basic_block(self, x, planes, stride):\n",
    "        out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n",
    "        zero_pads = torch.zeros(out.size(0), planes - out.size(1), out.size(2),\n",
    "                                out.size(3), out.size(4))\n",
    "        if isinstance(out.data, torch.FloatTensor):\n",
    "            zero_pads = zero_pads\n",
    "\n",
    "        out = torch.cat([out.data, zero_pads], dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes * block.expansion:\n",
    "            if shortcut_type == 'A':\n",
    "                downsample = partial(self._downsample_basic_block,\n",
    "                                     planes=planes * block.expansion,\n",
    "                                     stride=stride)\n",
    "            else:\n",
    "                downsample = nn.Sequential(\n",
    "                    conv1x1x1(self.in_planes, planes * block.expansion, stride),\n",
    "                    nn.BatchNorm3d(planes * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(in_planes=self.in_planes,\n",
    "                  planes=planes,\n",
    "                  stride=stride,\n",
    "                  downsample=downsample))\n",
    "        self.in_planes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.in_planes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, tab):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        if not self.no_max_pool:\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        tab = self.ln1(tab)\n",
    "        tab = self.relu(tab)\n",
    "        tab = self.ln2(tab)\n",
    "        tab = self.relu(tab)\n",
    "        tab = self.ln3(tab)\n",
    "        tab = self.relu(tab)\n",
    "        \n",
    "        x = torch.cat((x, tab), dim=1)\n",
    "        x= self.relu(x)\n",
    "        \n",
    "        x = self.ln4(x)        \n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def ResNet18(in_channels, num_classes):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], in_channels=in_channels, num_classes=num_classes)\n",
    "\n",
    "\n",
    "def generate_model(model_depth, **kwargs):\n",
    "    assert model_depth in [10, 18, 34, 50, 101, 152, 200]\n",
    "\n",
    "    if model_depth == 10:\n",
    "        model = ResNet(BasicBlock, [1, 1, 1, 1], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 18:\n",
    "        model = ResNet(BasicBlock, [2, 2, 2, 2], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 34:\n",
    "        model = ResNet(BasicBlock, [3, 4, 6, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 50:\n",
    "        model = ResNet(Bottleneck, [3, 4, 6, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 101:\n",
    "        model = ResNet(Bottleneck, [3, 4, 23, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 152:\n",
    "        model = ResNet(Bottleneck, [3, 8, 36, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 200:\n",
    "        model = ResNet(Bottleneck, [3, 24, 36, 3], get_inplanes(), **kwargs)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = generate_model(18, n_input_channels=1, widen_factor=1.0, \n",
    "                       n_classes=1, img_contribution=10, tabular_val=len(required_columns), tabular_contribution=10)\n",
    "model = model.double()\n",
    "print(model)\n",
    "\n",
    "\n",
    "print('Total Parameters:',\n",
    "      sum([torch.numel(p) for p in model.parameters()]))\n",
    "print('Trainable Parameters:',\n",
    "      sum([torch.numel(p) for p in model.parameters() if p.requires_grad]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cad353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_x, label_x = tv_set[0]\n",
    "#img_x.shape\n",
    "\n",
    "#img_x = img_x.unsqueeze(dim=0)\n",
    "#img_x = img_x.unsqueeze(dim=0)\n",
    "#output = model(img_x)\n",
    "#output.shape\n",
    "\n",
    "#output = model(img_x)\n",
    "#output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f2bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, loaders, optimizer, criterion, epochs=500, dev='cpu', save_param = True, model_name=\"adni_multi_input\"):\n",
    "    torch.manual_seed(myseed)\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        net = net.to(dev)\n",
    "        #print(net)\n",
    "        # Initialize history\n",
    "        history_loss = {\"train\": [], \"val\": [], \"test\": []}\n",
    "        history_accuracy = {\"train\": [], \"val\": [], \"test\": []}\n",
    "        # Store the best val accuracy\n",
    "        best_val_accuracy = 0\n",
    "\n",
    "        # Process each epoch\n",
    "        for epoch in range(epochs):\n",
    "            # Initialize epoch variables\n",
    "            sum_loss = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "            sum_accuracy = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "            # Process each split\n",
    "            for split in [\"train\", \"val\", \"test\"]:\n",
    "                if split == \"train\":\n",
    "                    net.train()\n",
    "                else:\n",
    "                    net.eval()\n",
    "                # Process each batch\n",
    "                for (image, tabular, labels) in loaders[split]:\n",
    "                    # Move to CUDA\n",
    "                    images = image.to(dev)\n",
    "                    tabular = tabular.to(dev)\n",
    "                    labels = labels.to(dev)\n",
    "                    # Reset gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    # Compute output\n",
    "                    pred = net(image, tabular)\n",
    "                    #pred = pred.squeeze(dim=1) # Output shape is [Batch size, 1], but we want [Batch size]\n",
    "                    labels = labels.unsqueeze(1)\n",
    "                    labels = labels.float()\n",
    "                    loss = criterion(pred, labels)\n",
    "                    # Update loss\n",
    "                    sum_loss[split] += loss.item()\n",
    "                    # Check parameter update\n",
    "                    if split == \"train\":\n",
    "                        # Compute gradients\n",
    "                        loss.backward()\n",
    "                        # Optimize\n",
    "                        optimizer.step()\n",
    "                    # Compute accuracy\n",
    "                    #pred_labels = pred.argmax(1) + 1\n",
    "                    pred_labels = (pred >= 0).long() # Binarize predictions to 0 and 1\n",
    "                    batch_accuracy = (pred_labels == labels).sum().item()/image.size(0)\n",
    "                    # Update accuracy\n",
    "                    sum_accuracy[split] += batch_accuracy\n",
    "                scheduler.step()\n",
    "            # Compute epoch loss/accuracy\n",
    "            epoch_loss = {split: sum_loss[split]/len(loaders[split]) for split in [\"train\", \"val\", \"test\"]}\n",
    "            epoch_accuracy = {split: sum_accuracy[split]/len(loaders[split]) for split in [\"train\", \"val\", \"test\"]}\n",
    "\n",
    "            # Update history\n",
    "            for split in [\"train\", \"val\", \"test\"]:\n",
    "                history_loss[split].append(epoch_loss[split])\n",
    "                history_accuracy[split].append(epoch_accuracy[split])\n",
    "                \n",
    "            writer.add_scalar(\"Train Loss\", epoch_loss['train'], epoch)\n",
    "            writer.add_scalar(\"Valid Loss\", epoch_loss['val'], epoch)\n",
    "            writer.add_scalar(\"Test Loss\", epoch_loss['test'], epoch)\n",
    "            writer.add_scalar(\"Train Accuracy\", epoch_accuracy['train'], epoch)\n",
    "            writer.add_scalar(\"Valid Accuracy\", epoch_accuracy['val'], epoch)\n",
    "            writer.add_scalar(\"Test Accuracy\", epoch_accuracy['test'], epoch)\n",
    "            writer.add_scalar(\"ETA\", time.time()-start_time, epoch)\n",
    "            \n",
    "            # Print info\n",
    "            print(f\"Epoch {epoch+1}:\",\n",
    "                  f\"TrL={epoch_loss['train']:.4f},\",\n",
    "                  f\"TrA={epoch_accuracy['train']:.4f},\",\n",
    "                  f\"VL={epoch_loss['val']:.4f},\",\n",
    "                  f\"VA={epoch_accuracy['val']:.4f},\",\n",
    "                  f\"TeL={epoch_loss['test']:.4f},\",\n",
    "                  f\"TeA={epoch_accuracy['test']:.4f},\",\n",
    "                  f\"LR={optimizer.param_groups[0]['lr']:.5f},\"\n",
    "                  f\"s={time.time()-start_time:.4f},\")\n",
    "\n",
    "            \n",
    "           # Store params at the best validation accuracy\n",
    "            if save_param:\n",
    "                if (epoch_accuracy['val'] > best_val_accuracy):\n",
    "                    print(f\"\\nFound new best: {epoch_accuracy['val']} - Saving best at epoch: {epoch+1}\")\n",
    "                    PATH = os.path.join(model_name,\"best_val.pth\")\n",
    "                    try:\n",
    "                        state_dict = net.module.state_dict()\n",
    "                    except AttributeError:\n",
    "                        state_dict = net.state_dict()\n",
    "                        \n",
    "                    torch.save({\n",
    "                                'epoch': epoch,\n",
    "                                'model_state_dict': state_dict,\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'loss': loss,\n",
    "                                }, PATH)\n",
    "                    best_val_accuracy = epoch_accuracy['val']\n",
    "            \n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    finally:\n",
    "        # Plot loss\n",
    "        plt.title(\"Loss\")\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            plt.plot(history_loss[split], label=split)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        # Plot accuracy\n",
    "        plt.title(\"Accuracy\")\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            plt.plot(history_accuracy[split], label=split)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f6ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "\n",
    "    if isinstance(m, nn.Conv3d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d01a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test set for x-tests\n",
    "outpath = os.path.join(f\"runs\",f\"adni{adni_num}_{experiment_type}\",f\"{experiment_name}\")\n",
    "if not os.path.exists(outpath):\n",
    "    os.makedirs(outpath)\n",
    "\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(myseed)\n",
    "test_loader = DataLoader(test_data,  batch_size=8, num_workers=1, drop_last=False, shuffle=False, generator=generator)\n",
    "\n",
    "tv_labels = adni_tv['labels'].tolist()\n",
    "skf = StratifiedKFold(n_splits = num_folds)\n",
    "\n",
    "for fold,(train_idx,val_idx) in enumerate(skf.split(tv_data, tv_labels)):\n",
    "    \n",
    "    writer = SummaryWriter(os.path.join(outpath,f\"{fold}\"), filename_suffix=f\"_E{epochs}\")\n",
    "    print('------------fold no---------{}----------------------'.format(fold))   \n",
    "    train_df = adni_tv.iloc[train_idx]\n",
    "    train_set = ImgDataset(adni_df=train_df)\n",
    "\n",
    "    val_df = adni_tv.iloc[val_idx]\n",
    "    val_set = ImgDataset(adni_df=val_df)\n",
    "    \n",
    "    train_loader = DataLoader(train_set, batch_size=8, drop_last=False)\n",
    "    val_loader = DataLoader(val_set, batch_size=8, drop_last=False)\n",
    "    \n",
    "    # Define dictionary of loaders\n",
    "    loaders = {\"train\": train_loader,\n",
    "               \"val\": val_loader,\n",
    "               \"test\": test_loader}\n",
    "\n",
    "    # Model Params\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "    # Define a loss \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr = 0.01, epochs=epochs, steps_per_epoch=len(train_loader))\n",
    "    \n",
    "    \n",
    "    #model, optimizer = ipex.optimize(model, optimizer=optimizer,dtype=torch.double)\n",
    "    #model = model.float()\n",
    "    # Train model\n",
    "    train(model, loaders, optimizer, criterion, epochs=epochs, dev=dev, model_name=os.path.join(outpath,f\"{fold}\"))\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    model.apply(reset_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deabfbb9",
   "metadata": {},
   "source": [
    "### TEST on Cross datasets\n",
    "\n",
    "- Se giri uno dei tre esperimenti da zero, ricordati di aggiornare il file contenente il test-set in tutti gli altri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082657ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_dir = f'X-TEST_{test_data_suffix}'\n",
    "\n",
    "external_test_names = ['test_adni1', 'test_adni2', 'test_adni3']\n",
    "external_datal = {}\n",
    "# Load external test_sets\n",
    "for name in external_test_names:\n",
    "    \n",
    "    ext_test_path = os.path.join(outpath, x_test_dir ,f'{name}_{test_data_suffix}.pt')\n",
    "    loaded_test = torch.load(ext_test_path)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    test_loader = DataLoader(loaded_test,  batch_size=8, num_workers=4, drop_last=False, shuffle=False, generator=generator)\n",
    "    external_datal[name] = test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929fb1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_results = {}\n",
    "\n",
    "for fold in list(range(num_folds)):\n",
    "#for fold in [0]:\n",
    "    fold_results = {}\n",
    "    #saved_test = torch.load(os.path.join(outpath, f'test_adni{adni_num}.pt') )\n",
    "    best_model_path = os.path.join(outpath, f\"{fold}\",\"best_val.pth\")\n",
    "\n",
    "    model = generate_model(18, n_input_channels=1, widen_factor=1.0, \n",
    "                           n_classes=1, img_contribution=10, tabular_val=len(required_columns), \n",
    "                           tabular_contribution=10)\n",
    "    \n",
    "    model = model.double()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "    if False:\n",
    "        from collections import OrderedDict\n",
    "        new_state_dict = OrderedDict()\n",
    "\n",
    "        for k, v in state_dict.items():\n",
    "            name = k[7:] # remove module.\n",
    "            new_state_dict[name] = v\n",
    "        #load params\n",
    "        model.load_state_dict(new_state_dict)\n",
    "    else:\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    net = model.eval()\n",
    "\n",
    "    sum_loss = {x_test:0 for x_test in external_datal }\n",
    "    sum_accuracy = {x_test:0 for x_test in external_datal }\n",
    "\n",
    "    for x_test in external_datal:\n",
    "        test_loader = external_datal[x_test]\n",
    "        \n",
    "        for (image, tabular, labels) in test_loader:\n",
    "            # Move to CUDA\n",
    "            images = image.to(dev)\n",
    "            tabular = tabular.to(dev)\n",
    "            labels = labels.to(dev)\n",
    "\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Compute output\n",
    "            pred = net(image, tabular)\n",
    "            #pred = pred.squeeze(dim=1) # Output shape is [Batch size, 1], but we want [Batch size]\n",
    "            labels = labels.unsqueeze(1)\n",
    "            labels = labels.float()\n",
    "            loss = criterion(pred, labels)\n",
    "\n",
    "            # Update loss\n",
    "            sum_loss[x_test] += loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            #pred_labels = pred.argmax(1) + 1\n",
    "            pred_labels = (pred >= 0.0).long() # Binarize predictions to 0 and 1\n",
    "            batch_accuracy = (pred_labels == labels).sum().item()/tabular.size(0)\n",
    "            # Update accuracy\n",
    "            sum_accuracy[x_test] += batch_accuracy\n",
    "\n",
    "        scheduler.step()\n",
    "        # Compute epoch loss/accuracy\n",
    "\n",
    "        loss = {x_test: sum_loss[x_test]/len(external_datal[x_test]) for x_test in list(external_datal.keys())}\n",
    "        accuracy = {x_test: sum_accuracy[x_test]/len(external_datal[x_test]) for x_test in list(external_datal.keys())}\n",
    "        \n",
    "        fold_results['loss'] = loss\n",
    "        fold_results['accuracy'] = accuracy\n",
    "        x_test_results[f\"{fold}\"] = fold_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6046bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "decimals = 4\n",
    "final_summary={}\n",
    "for x_test in external_datal.keys():  \n",
    "    local_summary = []\n",
    "    for f in x_test_results:\n",
    "        acc = x_test_results[f]['accuracy'][x_test]\n",
    "        local_summary.append(acc)\n",
    "        \n",
    "    final_summary[x_test] = local_summary \n",
    "    print(f\"{x_test}, \\\n",
    "          \\n Values = {local_summary}, \\\n",
    "          \\n avg = {round(np.average(local_summary), decimals)}, std = {round(np.std(local_summary),decimals)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f868ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls runs/adni3_multi_input/run1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2373de4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the tensorboard aftern enabling the port fwd using same port: localhost:6002\n",
    "!tensorboard --logdir runs/adni3_multi_input/run1 --bind_all --load_fast=false --port=6003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576baca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
